---
title: "phrases"
author: "Meilin"
date: "7/26/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##### This markdown visualizes the common flow of words in tweets by using quanteda package to catch phrases. These visualizations are separated into: the full noRT dataset, tweets about soil, forest, and rangeland health, and then repeats this for the top 100 tweets (based on their RT count)

```{r, include=FALSE}
# load packages
library(tidyverse)
library(tidytext)
library(stringr)
library(ggraph)
library(igraph)
library(tm)
library(NLP)
library(quanteda)
library(SnowballC)
source("../text_analysis_functions.R")

# load data
noRT <- read.csv("/home/shares/soilcarbon/Twitter/Merged_v2/twitter_merged_noRT_v2.csv", stringsAsFactors = FALSE) 
```

```{r, include=FALSE}
#clean data to remove numbers, usernames, websites, non-ASCII characters and outlier
noRT_clean <- removeNumbers(noRT$text)
noRT_clean <- gsub("@\\w+","",noRT_clean)
noRT_clean <- gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", "", noRT_clean)
noRT_clean <- gsub("#\\s+","", noRT_clean)
noRT_clean <- gsub("amp", "", noRT_clean)
noRT_clean <- gsub("[^\x01-\x7F]", "", noRT_clean)

noRT$text <- noRT_clean
noRT <- noRT %>% 
  filter(source != "Twittascope") 

# to remove the pope
noRT <- noRT %>%
  arrange(-retweet_count) %>%
  filter(screen_name != "Pontifex")

# to remove all india related tweets
noRT_india <- flag_india(noRT)
noRT_no_india <- noRT_india %>% 
  filter(is_india == 0)

```

```{r, include=FALSE}
# select top 25 and 100 tweets based on their retweet count
# use a sample in the following code
top_25_noRT <- noRT_no_india %>%
  arrange(-retweet_count) %>%
  head(25)

top_100_noRT <- noRT_no_india %>% 
  arrange(-retweet_count) %>% 
  head(100)

rest_noRT <- noRT_no_india %>% 
  arrange(-retweet_count) %>%
  slice(101:n())
```

```{r}
# tokenize the text, remove punctuation, symbols and stop words
toks <- tokens(top_100_noRT$text)
toks <- tokens(toks, remove_punct = T, remove_symbols = T)
toks_nostop <- tokens_select(toks, pattern = stopwords('en'), selection = 'remove')

# select keyword "soil" in context
kwic(toks_nostop, pattern = "soil")
```
```{r}
# select top features in top 100 noRT using dfm
dfm_noRT <- dfm(toks_nostop)
top_100_feat <- topfeatures(dfm_noRT, 20)
top_100_feat
```

```{r}
# see if selective n-gram works
toks_neg_bigram <- tokens_compound(toks_nostop, pattern = phrase('soil *'))
toks_neg_bigram_select <- tokens_select(toks_neg_bigram, pattern = phrase('soil_*'))
toks_neg_bigram_select
```

```{r}
# generate network of top 100 noRT using fcm (feature co-occurrence matrix)
fcm_noRT <- fcm(dfm_noRT)
feat <- names(topfeatures(fcm_noRT, 50))
fcm_noRT_select <- fcm_select(fcm_noRT, pattern = feat)

size <- log (colSums(dfm_select(dfm_noRT, feat)))
set.seed(144)
textplot_network(fcm_noRT_select, min_freq = 0.8, vertex_size = size/ max(size) * 3)

```

***
```{r}
# collocation analysis
# detect phrases

# creat tokens with the full noRT dataset
toks_full <- tokens(noRT_no_india$text)

# remove punctuation, symbols, numbers, and spaces
toks_full <- tokens(toks_full, remove_punct = T, remove_symbols = T, remove_numbers = T)
# remove the stop words
toks_nostop_full <- tokens_select(toks_full, pattern = stopwords('english'), selection = 'remove')
# covert to stem words
toks_nostop_full <- tokens_wordstem(toks_nostop_full)

tstat_col_caps <- tokens_select(toks_nostop_full, pattern = '^[A-Z]', 
                                valuetype = 'regex', 
                                case_insensitive = T, 
                                padding = TRUE) %>% 
  textstat_collocations(min_count = 100)
  #textstat_collocations(min_count =  100, size = 3) # collocations of 3 words

head(tstat_col_caps, 100) %>% arrange(desc(count)) %>% filter(collocation != "soil health" & collocation != "healthi soil"& collocation != "regen agricultur" & collocation != "soil fertil" & collocation != "soil qualiti")
```


# Compare the phrases from the top 25, top 100 to the rest
```{r}
toks_25 <- tokens(top_25_noRT$text)
toks_25 <- tokens(toks_25, remove_punct = T, remove_symbols = T)
toks_nostop_25 <- tokens_select(toks_25, pattern = stopwords('en'), selection = 'remove')
# stem words
toks_nostop_25 <- tokens_wordstem(toks_nostop_25)

tstat_col_caps_25 <- tokens_select(toks_nostop_25, pattern = '^[A-Z]', 
                                valuetype = 'regex', 
                                case_insensitive = T, 
                                padding = TRUE) %>% 
  textstat_collocations(min_count = 2)
  #textstat_collocations(min_count =  2, size = 3) # collocations of 3 words

head(tstat_col_caps_25, 20) %>% arrange(desc(count)) %>% filter(collocation != "soil health" & collocation != "healthi soil"& collocation != "regen agricultur" & collocation != "soil fertil" & collocation != "soil qualiti")
```

```{r}
toks_100 <- tokens(top_100_noRT$text)
toks_100 <- tokens(toks_100, remove_punct = T, remove_symbols = T)
toks_nostop_100 <- tokens_select(toks_100, pattern = stopwords('en'), selection = 'remove')
# stem words
toks_nostop_100 <- tokens_wordstem(toks_nostop_100)

tstat_col_caps_100 <- tokens_select(toks_nostop_100, pattern = '^[A-Z]', 
                                valuetype = 'regex', 
                                case_insensitive = T, 
                                padding = TRUE) %>% 
  textstat_collocations(min_count = 2)
  #textstat_collocations(min_count =  2, size = 3) # collocations of 3 words

head(tstat_col_caps_100, 50) %>% arrange(desc(count)) %>% filter(collocation != "soil health" & collocation != "healthi soil"& collocation != "regen agricultur" & collocation != "soil fertil" & collocation != "soil qualiti")
```

```{r}
# compared to the rest (exclude the top 100)
toks_rest <- tokens(rest_noRT$text)
toks_rest <- tokens(toks_rest, remove_punct = T, remove_symbols = T)
toks_nostop_rest <- tokens_select(toks_rest, pattern = stopwords('en'), selection = 'remove')
# stem words
toks_nostop_rest <- tokens_wordstem(toks_nostop_rest)

tstat_col_caps_rest <- tokens_select(toks_nostop_rest, pattern = '^[A-Z]', 
                                valuetype = 'regex', 
                                case_insensitive = T, 
                                padding = TRUE) %>% 
  #textstat_collocations(min_count = 100)
  textstat_collocations(min_count =  100, size = 3) # collocations of 3 words

head(tstat_col_caps_rest, 100) %>% arrange(desc(count)) %>% filter(collocation != "soil health" & collocation != "healthi soil"& collocation != "regen agricultur" & collocation != "soil fertil" & collocation != "soil qualiti")

```

***

```{r}
# select top features in whole noRT using dfm
dfm_noRT_full <- dfm(toks_nostop_full)
top_feat <- topfeatures(dfm_noRT_full, 20)
top_feat
```

```{r}
# generate network of whole noRT using fcm (feature co-occurrence matrix)
fcm_noRT_full <- fcm(dfm_noRT_full)
feat <- names(topfeatures(fcm_noRT_full, 50))
fcm_noRT_select_full <- fcm_select(fcm_noRT_full, pattern = feat)

size <- log (colSums(dfm_select(dfm_noRT_full, feat)))
set.seed(144)
textplot_network(fcm_noRT_select_full, min_freq = 0.8, vertex_size = size/ max(size) * 3)
```
